- Create mappings as part of the index:
$ bin/curl -XPUT 127.0.0.1:9200/movies/_doc/109487 -d '
{
"genre" :
["IMAX","Sci-Fi"],
"title" : "Interstellar",
"year" : 2014
}'
{"_index":"movies","_type":"_doc","_id":"109487","_version":1,"result":"created","_shards":{"total":2,"successful":1,"failed":0},"_seq_no":0,"_primary_term":1}

- Get created mappings, as part of the index:
$ bin/curl -XGET 127.0.0.1:9200/movies/_mapping
{"movies":{"mappings":{"properties":{"genre":{"type":"text","fields":{"keyword":{"type":"keyword","ignore_above":256}}},"title":{"type":"text","fields":{"keyword":{"type":"keyword","ig
nore_above":256}}},"year":{"type":"long"}}}}}

- Insert the movie and apply that mapping to it:
	- this movie will be of type "_doc" and _id "109487"
$ bin/curl -XPOST 127.0.0.1:9200/movies/_doc/109487 -d '
 {
	"genre" : ["IMAX","Sci-Fi"],
	"title" : "Interstellar",
	"year": 2014
 }'
 
result: {"_index":"movies","_type":"_doc","_id":"109487","_version":2,"result":"updated","_shards":{"total":2,"successful":1,"failed":0},"_seq_no":1,"_primary_term":1}


- Get the inserted data to index:
$ bin/curl -XGET 127.0.0.1:9200/movies/_search?pretty

- Insert data with a bulk operation
	+ fetch data from wget http://media.sundog-soft.com/es7/movies.json
	+ certify that the file is the one we need:
		$ cat movies.json
	$ bin/curl -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @movies.json

	
- Update existing index	
	* in elasticsearch, documents are immutable
		workaround: 
			+ a new document is created with an incremented_version
			+ old document is marked for deletion
			+ to update a document, we do a copy of that doc which will have an incremented version
			+ id and version togheter mus be unique
			+ elasticsearch will use the most recent version created
			+ elasticsearch will do a cleanup, and the old version will go away
			+ for this operation, we use a POST
			
			* in the folowing example, we are going to update the title for a doc with id 109487
				- first update index:
					$ bin/curl -XPUT 127.0.0.1:9200/movies/_doc/109487?pretty -d '
						{
						"genres": ["IMAX","Sci-Fi"],
						"title": "Intersteller foo",
						"year": 2014
						}'
				- certify the index was updated
					$ bin/curl -XGET 127.0.0.1:9200/movies/_doc/109487?pretty
				- do a partial update, just changing the title:
					$ bin/curl -XPOST 127.0.0.1:9200/movies/_doc/109487/_update -d '
						{
						"doc": {
						"title": "Interstellar"
						}
						}'
					result:	{"_index":"movies","_type":"_doc","_id":"109487","_version":4,"result":"updated","_shards":{"total":2,"successful":1,"failed":0},"_seq_no":7,"_primary_term":1}
					
				- get that doc to certify version was updated to 4:
					$ bin/curl -XGET 127.0.0.1:9200/movies/_doc/109487?pretty
					
- Delete existing documents
	* identifies which index you and which document id
		- fetch document to be deleted
			$ bin/curl -XGET 127.0.0.1:9200/movies/_search?q=Dark
		- delete document
			$ bin/curl -XDELETE 127.0.0.1:9200/movies/_doc/58559?pretty
		result:
			{
			  "_index" : "movies",
			  "_type" : "_doc",
			  "_id" : "58559",
			  "_version" : 2,
			  "result" : "deleted",
			  "_shards" : {
				"total" : 2,
				"successful" : 1,
				"failed" : 0
			  },
			  "_seq_no" : 8,
			  "_primary_term" : 1
			}
		
- Dealing with concurrency:
	Try to delete document:
		$ bin/curl -XPUT "127.0.0.1:9200/movies/_doc/109487?if_seq_no=7&if_primary_term=1" -d '
		 {
		 "genres": ["IMAX", "Sci-Fi"],
		 "title": "Interstellar foo",
		 "year": 2014
		 }'
		 

		 
Analyser & Tokenyzer
	- search for a document that has Star Trek in the title	
		$ bin/curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '
			{
			"query": {
			"match": {
			"title": "Star Treck"
			}
			}
			}'
			With this analyser, we have a partial match. 
			We can see the score for the match-> "_score" : 0.9579736,
			Best match will be on top of the result
			The search term "Star Trek" got brought broken up into two unique search terms, and usin the inverted index, all the result found, match the term "star"
			
	- search by renges
		$ bin/curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '
			{
			"query": {
			"match_phrase": {
			"genre": "sci"
			}
			}
			}'
			with this analyser, the term "sci" was used as a match. I lowercase everything to normalize that data, so upper case or lower case side doesn't matter, it is using it just like a title, so partial hits are ok as well
			
		* to have genre more tightly defined, with only one hit for the actual phrase sci fi. 
		* for this, we have to change our mappings and reindex
			$ bin/curl -XDELETE 127.0.0.1:9200/movies/
		* define a new mapping and reindex our data	
			$ bin/curl -XPUT 127.0.0.1:9200/movies -d '
				{
				"mappings": {
				"properties": {
				"id": {"type": "integer"},
				"year": {"type": "date"},
				"genre": {"type": "keyword"},   => we are only going to do exact matches, no analyzer will be run on this field. It will be casesensitive, the whole word
				"title": {"type": "text", "analyzer": "english"} => will have an analyser applied, so we can do partial matches, normalizing for lowercase. We can also specify the specific analyzer for this field, for example "analyzer": "english". We can apply stop words and synonyms that might be specific for English language 
				}
				}
				}
				}'
				result:
				{"acknowledged":true,"shards_acknowledged":true,"index":"movies"}
		* now, we can rindex data
			$ bin/curl -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @movies.json
		* we can now search for exact match "Sci-Fi", and we should get a match:
			$ bin/curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '
				{
				"query": {
				"match": {
				"genre": "Sci-Fi"
				}
				}
				}'
		* we can also fetch title as "star wars"
			$ bin/curl -XGET 127.0.0.1:9200/movies/_search?pretty -d '
				{
				"query": {
				"match": {
				"title": "star wars"
				}
				}
				}'
				* in thie case, we are using partial match, and we should get a result
					- star wars appear in the first place, because it has higher score than star trek
					
					
Parent / child relationship
	- create a new index (series) with a parent / child relationship in the mappings
		*$ bin/curl -XPUT 127.0.0.1:9200/series -d '
			{
			"mappings": {
			"properties": {
			"film_to_franchise": {
			"type": "join",			=> join films to franchise
			"relations": {"franchise": "film"}		=> franchise is parent, films are childs
			}
			}
			}
			}'
			{"acknowledged":true,"shards_acknowledged":true,"index":"series"}
	- download series 
		$ wget  http://media.sundog-soft.com/es7/series.json
	- add series as bulk
		$ bin/curl -XPUT 127.0.0.1:9200/_bulk?pretty --data-binary @series.json
		
	- retrieve all the series associated witr star wars franchise
		$ bin/curl -XGET 127.0.0.1:9200/series/_search?pretty -d '
			{
			"query": {
			"has_parent": {"parent_type": "franchise",		=> we can have multiple parents
			"query": {
			"match": {		=> we can have a subquery to match
			"title": "Star Wars"
			}
			}
			}
			}
			}'
			
	- find a specific franchise associated with a film
		$ bin/curl -XGET 127.0.0.1:9200/series/_search?pretty -d '
			{
			"query": {
			"has_child": {
			"type": "film",
			"query": {
			"match": {
			"title": "The Force Awakens"
			}
			}
			}
			}}'
			
			

Flaterned Datatypes:
	- use the following commands, found in:
		http://media.sundog-soft.com/es/flattened.txt
	- create an indexe called demo-default:
		curl -XPUT "http://127.0.0.1:9200/demo-default/_doc/1" -d'{
		  "message": "[5592:1:0309/123054.737712:ERROR:child_process_sandbox_support_impl_linux.cc(79)] FontService unique font name matching request did not receive a response.",
		  "fileset": {
			"name": "syslog"
		  },
		  "process": {
			"name": "org.gnome.Shell.desktop",
			"pid": 3383
		  },
		  "@timestamp": "2020-03-09T18:00:54.000+05:30",
		  "host": {
			"hostname": "bionic",
			"name": "bionic"
		  }
		}'
	- see what mappings were created automatically
		$ bin/curl -XGET "http://127.0.0.1:9200/demo-default/_mapping?pretty=true"
		- elasticsearch assigned types for each of the fields
		- flatern data types is designed to handle the use case of unknown or large numbers of inner fields occouring in a document
			+ this can cause search cluster to go down
			+ each fiels has an associated mapping type in its index, and can be automatically assigned
			+ elasticsearch holds the mapping information of every index in the cluster state. Cluster state includes information such as index mappings, the node details, etc.
			+ we can query the cluster state by using the cluster state API:
				curl -XGET "http://127.0.0.1:9200/_cluster/state?pretty=true" >> es-cluster-state.json
				open the es-cluster-state.json file with nano, enter CRTL+W to searh for "demo-default"
			+ updatig the mapping cause updates to be sent from Master Node to other nodes to synchronize the mappings	
				- this can end up in having poor performance and causeing the cluster going down
				- this is called mapping explosion
				- to prevent this, flat end to data type was introduced
				- this data type maps the entire object along with its inner fields into a single field
				- if a field contains a inner field, the flat data type maps the parent field as a single type named flattened and the inner fields don't appear in the mappings at all, reducing the total map fields
			
			+ for that, let's create a new index called demo-flattened
			+ create a mapping for that index:
				curl -XPUT "http://127.0.0.1:9200/demo-flattened/_mapping" -d'{
				  "properties": {
					"host": {
					  "type": "flattened"
					}
				  }
				}'
				


Mapping excetions
	* fetch commands from:
		http://media.sundog-soft.com/es/exceptions.txt
	* create an index called microservices:
		curl --request PUT 'http://localhost:9200/microservice-logs' \
		--data-raw '{
		   "mappings": {
			   "properties": {
				   "timestamp": { "type": "date"  },
				   "service": { "type": "keyword" },
				   "host_ip": { "type": "ip" },
				   "port": { "type": "integer" },
				   "message": { "type": "text" }
			   }
		   }
		}'
	* what happens if we insert data with wrong value for a type (port is an inter and we are passing a string)
		$ bin/curl --request POST 'http://localhost:9200/microservice-logs/_doc?pretty' \
		--data-raw '{"timestamp": "2020-04-11T12:34:56.789Z", "service": "XYZ", "host_ip": "10.0.2.15", "port": "15000", "message": "Hello!" }'
	result: it does not give an error
	
	* what if we try port: "NONE"
		$ bin/curl --request POST 'http://localhost:9200/microservice-logs/_doc?pretty' \
		--data-raw '{"timestamp": "2020-04-11T12:34:56.789Z", "service": "XYZ", "host_ip": "10.0.2.15", "port": "NONE", "message": "I am not well!" }'
	return; mapper_parsing_exception
	
	* we can solve the problem by defining a ignore or malformed  mapping parameter
		$ bin/curl --request POST 'http://localhost:9200/microservice-logs/_close'
		$ bin/curl --location --request PUT 'http://localhost:9200/microservice-logs/_settings' \
			> --data-raw '{
			>    "index.mapping.ignore_malformed": true
			> }'

	* $ bin/curl --request POST 'http://localhost:9200/microservice-logs/_open'
	* if we run the same command which causes an exception, it will actionally work.
	* we can see that port field is ignored
		$ bin/curl 'http://localhost:9200/microservice-logs/_doc/W9RswncBL0W-VOV5MFcn?pretty'
			{
			  "_index" : "microservice-logs",
			  "_type" : "_doc",
			  "_id" : "W9RswncBL0W-VOV5MFcn",
			  "_version" : 1,
			  "_seq_no" : 1,
			  "_primary_term" : 2,
			  "_ignored" : [
				"port"
			  ],
			  "found" : true,
			  "_source" : {
				"timestamp" : "2020-04-11T12:34:56.789Z",
				"service" : "XYZ",
				"host_ip" : "10.0.2.15",
				"port" : "NONE",
				"message" : "I am not well!"
			  }
			}

		






